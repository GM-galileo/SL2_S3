{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adapted-standing",
   "metadata": {},
   "source": [
    "# <font color=blue>**TAREA 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thousand-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-veteran",
   "metadata": {},
   "source": [
    "## <font color=blue>**Parte 2.1 - Deteccion de Anomalias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-cradle",
   "metadata": {},
   "source": [
    "- Usando el dataset de estaturas(en google drive) aplicar detección de anomalías\n",
    "    - Usar como training-set la primera pestaña “normales” (y=0)\n",
    "    - La segunda pestaña “valtest(normales)” contiene datos normales(y=0):\n",
    "        - Usar la mitad para cross-validation\n",
    "        - Usar la mitad para testing\n",
    "    - La tercera pestaña “valtest(anomalias)” contiene anomalías(y=1):\n",
    "        - Usar la mitad para cross-validation \n",
    "        - Usar la mitad para testing\n",
    "- Los datos de cross-validation deben ser usados para selección de hyper-parámetros(por ejemplo el umbral epsilon) y/o selección de transformaciones a aplicar a las variables.\n",
    "- Usar los datos de testing para reportar las métricas de evaluación apropiadas\n",
    "- Es permitido usar librerías estadísticas(por ejemplo scipy.stats) para cosas como:\n",
    "    - Estimación de parámetros(función de densidad)\n",
    "    - Cálculo de densidades y/o probabilidades con la función de densidad estimada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colonial-scotland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estatura</th>\n",
       "      <th>Edad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.10</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Estatura   Edad\n",
       "0      0.25    2.4\n",
       "1    175.10    2.5\n",
       "2      0.15  250.0\n",
       "3    150.00   14.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EstaturasN = pd.read_excel('estaturas.xlsx','normales',dtype={'statura(metros)': float, 'Edad(años)': float})\n",
    "EstaturasVT = pd.read_excel('estaturas.xlsx','valtest(normales)',dtype={'statura(metros)': float, 'Edad(años)': float})\n",
    "EstaturasVA = pd.read_excel('estaturas.xlsx','valtest(anomalias)',dtype={'Estatura': float, 'Edad': float})\n",
    "EstaturasVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latest-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DE DATOS\n",
    "VT_Cross, VT_Test = train_test_split(EstaturasVT, test_size=0.5)\n",
    "VA_Cross, VA_Test = train_test_split(EstaturasVA, test_size=0.5)\n",
    "\n",
    "EstaturasC = np.vstack((np.array(VT_Cross),np.array(VA_Cross)))\n",
    "EstaturasT = np.vstack((np.array(VT_Test),np.array(VA_Test)))\n",
    "EstaturasL = np.concatenate((np.zeros(VT_Cross.shape[0]),np.ones(VA_Cross.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tender-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculo_MeanSD(Data):\n",
    "    Mean = np.mean(Data, axis=0)\n",
    "    StandardD = np.cov(Data.T)    \n",
    "    return Mean, StandardD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transparent-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculo_Densidad(Data,Mean, StandardD):\n",
    "    return multivariate_normal(mean =Mean, cov= StandardD ).pdf(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "educated-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeteccionAnomalia_Training(Data_Train, Data_Cross, Data_Label):\n",
    "    \n",
    "    m1, sd1 = Calculo_MeanSD(Data_Train)\n",
    "    prob = Calculo_Densidad(Data_Train, m1,sd1)\n",
    "    \n",
    "    prob_a = Calculo_Densidad(Data_Cross, m1, sd1)\n",
    "    \n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    f = 0\n",
    "    stepsize = (max(prob_a) - min(prob_a)) / 1000\n",
    "    epsilons = np.arange(min(prob_a),max(prob_a),stepsize)\n",
    "    for epsilon in np.nditer(epsilons):\n",
    "        predictions = (prob_a < epsilon) \n",
    "        f = f1_score(Data_Label, predictions,average='binary')\n",
    "        if f > best_f1:\n",
    "            best_f1 = f\n",
    "            best_epsilon = epsilon\n",
    "    \n",
    "    return best_f1, best_epsilon, m1, sd1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loved-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, epsilon, m, sd = DeteccionAnomalia_Training(EstaturasN, EstaturasC, EstaturasL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "american-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeteccionAnomalia_Prediction(Data_Test, epsilon, m, sd):\n",
    "    prob = Calculo_Densidad(Data_Test, m, sd)\n",
    "            \n",
    "    return prob<epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disabled-chrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False  True  True]\n"
     ]
    }
   ],
   "source": [
    "DeteccionAnomalia = DeteccionAnomalia_Prediction(EstaturasT, epsilon, m, sd)\n",
    "print(DeteccionAnomalia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relative-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalias\n",
      "[[1.5e-01 2.5e+02]\n",
      " [2.5e-01 2.4e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Anomalias\")\n",
    "print(EstaturasT[DeteccionAnomalia])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-israeli",
   "metadata": {},
   "source": [
    "## <font color=blue>**Parte 2.2 - Reduccion de Dimensionalidad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-acquisition",
   "metadata": {},
   "source": [
    "Usando el dataset de fashion MNIST realizar:\n",
    "- PCA sin sklearn para reducir a 2 dimensiones.\n",
    "    - Anotar la cantidad de varianza preservada.\n",
    "- t-sne con sklearn para reducir a 2 dimensiones.\n",
    "- Analizar ambas representaciones, comparar y concluir.\n",
    "- Aplicar clustering con sklearn sobre la representación reducida. \n",
    "- Ya que este dataset si posee etiquetas “y” (tipo de prenda) analizar si los clusters encontrados tienden a agrupar el mismo tipo de prenda o prendas similares(por ejemplo se puede graficar cada tipo de diferente color)\n",
    "- Agregar conclusiones finales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "FMist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = FMist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataset \n",
    "n, v1, v2 = train_images.shape\n",
    "train_imagesRS = train_images.reshape((n,v1*v2))\n",
    "train_imagesRS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-reading",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-syracuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "documentary-nigeria",
   "metadata": {},
   "source": [
    "**T-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_TSNE =  TSNE(n_components=2)\n",
    "Z = X_TSNE.fit_transform(train_imagesRS)\n",
    "#print(X_TSNE.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-intellectual",
   "metadata": {},
   "source": [
    "**KMeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans1 = KMeans(n_clusters=5, random_state=0).fit(Z)\n",
    "print(kmeans1.labels_)\n",
    "print(kmeans1.cluster_centers_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
